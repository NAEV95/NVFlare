{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5b0a0e2",
   "metadata": {},
   "source": [
    "# Federated Protein Embeddings and Task Model Fitting with BioNeMo\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE</b> This notebook was tested on a single A1000 GPU and is compatible with BioNeMo Framework v1.8. </div>\n",
    "\n",
    "This example notebook shows how to obtain protein learned representations in the form of embeddings using the ESM-1nv pre-trained model in a federated learning (FL) setting. The model is trained with NVIDIA's BioNeMo framework for Large Language Model training and inference. For more details, please visit NVIDIA BioNeMo Service at https://www.nvidia.com/en-us/gpu-cloud/bionemo.\n",
    "\n",
    "This example is based on NVIDIA BioNeMo Service [example](https://github.com/NVIDIA/BioNeMo/blob/main/examples/service/notebooks/task-fitting-predictor.ipynb) \n",
    "but runs inference locally (on the FL clients) instead of using BioNeMo's cloud API.\n",
    "\n",
    "This notebook will walk you through the task fitting workflow in the following sections:\n",
    "\n",
    "* Dataset sourcing & Data splitting\n",
    "* Federated embedding extraction\n",
    "* Training a MLP to predict subcellular location\n",
    "\n",
    "## Setup\n",
    "\n",
    "Ensure that you have read through the Getting Started section, can run the BioNeMo Framework docker container, and have configured the NGC Command Line Interface (CLI) within the container. It is assumed that this notebook is being executed from within the container.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE</b> Some of the cells below generate long text output.  We're using <pre>%%capture --no-display --no-stderr cell_output</pre> to suppress this output.  Comment or delete this line in the cells below to restore full output.</div>\n",
    "\n",
    "### Import and install all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14dc1b1f-42ab-4a10-959a-3d14163fa974",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting nvflare~=2.5.0rc\n",
      "  Downloading nvflare-2.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting cryptography>=36.0.0 (from nvflare~=2.5.0rc)\n",
      "  Downloading cryptography-43.0.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting Flask==3.0.2 (from nvflare~=2.5.0rc)\n",
      "  Downloading flask-3.0.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: Werkzeug==3.0.3 in /usr/local/lib/python3.10/dist-packages (from nvflare~=2.5.0rc) (3.0.3)\n",
      "Collecting Flask-JWT-Extended==4.6.0 (from nvflare~=2.5.0rc)\n",
      "  Downloading Flask_JWT_Extended-4.6.0-py2.py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting Flask-SQLAlchemy==3.1.1 (from nvflare~=2.5.0rc)\n",
      "  Downloading flask_sqlalchemy-3.1.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting SQLAlchemy==2.0.16 (from nvflare~=2.5.0rc)\n",
      "  Downloading SQLAlchemy-2.0.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.4 kB)\n",
      "Collecting grpcio>=1.62.1 (from nvflare~=2.5.0rc)\n",
      "  Downloading grpcio-1.66.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting gunicorn>=22.0.0 (from nvflare~=2.5.0rc)\n",
      "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: numpy<=1.26.4 in /usr/local/lib/python3.10/dist-packages (from nvflare~=2.5.0rc) (1.26.4)\n",
      "Collecting protobuf>=4.24.4 (from nvflare~=2.5.0rc)\n",
      "  Downloading protobuf-5.28.2-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: psutil>=5.9.1 in /usr/local/lib/python3.10/dist-packages (from nvflare~=2.5.0rc) (5.9.4)\n",
      "Requirement already satisfied: PyYAML>=6.0 in /usr/local/lib/python3.10/dist-packages (from nvflare~=2.5.0rc) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.28.0 in /usr/local/lib/python3.10/dist-packages (from nvflare~=2.5.0rc) (2.32.3)\n",
      "Requirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from nvflare~=2.5.0rc) (1.16.0)\n",
      "Requirement already satisfied: msgpack>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from nvflare~=2.5.0rc) (1.0.7)\n",
      "Collecting docker>=6.0 (from nvflare~=2.5.0rc)\n",
      "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting websockets>=10.4 (from nvflare~=2.5.0rc)\n",
      "  Downloading websockets-13.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting pyhocon (from nvflare~=2.5.0rc)\n",
      "  Downloading pyhocon-0.3.61-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from Flask==3.0.2->nvflare~=2.5.0rc) (3.1.3)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from Flask==3.0.2->nvflare~=2.5.0rc) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from Flask==3.0.2->nvflare~=2.5.0rc) (8.1.7)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from Flask==3.0.2->nvflare~=2.5.0rc) (1.8.2)\n",
      "Collecting PyJWT<3.0,>=2.0 (from Flask-JWT-Extended==4.6.0->nvflare~=2.5.0rc)\n",
      "  Downloading PyJWT-2.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy==2.0.16->nvflare~=2.5.0rc) (4.12.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy==2.0.16->nvflare~=2.5.0rc) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from Werkzeug==3.0.3->nvflare~=2.5.0rc) (2.1.4)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->nvflare~=2.5.0rc) (1.16.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker>=6.0->nvflare~=2.5.0rc) (1.26.16)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gunicorn>=22.0.0->nvflare~=2.5.0rc) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28.0->nvflare~=2.5.0rc) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28.0->nvflare~=2.5.0rc) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28.0->nvflare~=2.5.0rc) (2023.7.22)\n",
      "Requirement already satisfied: pyparsing<4,>=2 in /usr/local/lib/python3.10/dist-packages (from pyhocon->nvflare~=2.5.0rc) (3.1.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->nvflare~=2.5.0rc) (2.21)\n",
      "Downloading nvflare-2.5.0-py3-none-any.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading flask-3.0.2-py3-none-any.whl (101 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.3/101.3 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Flask_JWT_Extended-4.6.0-py2.py3-none-any.whl (22 kB)\n",
      "Downloading flask_sqlalchemy-3.1.1-py3-none-any.whl (25 kB)\n",
      "Downloading SQLAlchemy-2.0.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading cryptography-43.0.1-cp39-abi3-manylinux_2_28_x86_64.whl (4.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.66.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.28.2-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading websockets-13.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (164 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyhocon-0.3.61-py3-none-any.whl (25 kB)\n",
      "Downloading PyJWT-2.9.0-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: websockets, SQLAlchemy, PyJWT, pyhocon, protobuf, gunicorn, grpcio, Flask, docker, cryptography, Flask-SQLAlchemy, Flask-JWT-Extended, nvflare\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.60.0\n",
      "    Uninstalling grpcio-1.60.0:\n",
      "      Successfully uninstalled grpcio-1.60.0\n",
      "  Attempting uninstall: Flask\n",
      "    Found existing installation: Flask 3.0.3\n",
      "    Uninstalling Flask-3.0.3:\n",
      "      Successfully uninstalled Flask-3.0.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf 23.12.0 requires numba<0.58,>=0.57, but you have numba 0.60.0 which is incompatible.\n",
      "cudf 23.12.0 requires numpy<1.25,>=1.21, but you have numpy 1.26.4 which is incompatible.\n",
      "cudf 23.12.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.2.2 which is incompatible.\n",
      "cudf 23.12.0 requires protobuf<5,>=4.21, but you have protobuf 5.28.2 which is incompatible.\n",
      "dask-cudf 23.12.0 requires numpy<1.25,>=1.21, but you have numpy 1.26.4 which is incompatible.\n",
      "dask-cudf 23.12.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.2.2 which is incompatible.\n",
      "nvidia-pytriton 0.4.0 requires pyzmq~=23.0, but you have pyzmq 26.1.0 which is incompatible.\n",
      "triton-model-navigator 0.7.4 requires mpmath<1.0.0, but you have mpmath 1.3.0 which is incompatible.\n",
      "triton-model-navigator 0.7.4 requires onnx~=1.14.0, but you have onnx 1.16.0 which is incompatible.\n",
      "triton-model-navigator 0.7.4 requires protobuf<3.21.0,>=3.18.3, but you have protobuf 5.28.2 which is incompatible.\n",
      "wandb 0.15.6 requires protobuf!=4.21.0,<5,>=3.19.0; python_version > \"3.9\" and sys_platform == \"linux\", but you have protobuf 5.28.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Flask-3.0.2 Flask-JWT-Extended-4.6.0 Flask-SQLAlchemy-3.1.1 PyJWT-2.9.0 SQLAlchemy-2.0.16 cryptography-43.0.1 docker-7.1.0 grpcio-1.66.1 gunicorn-23.0.0 nvflare-2.5.0 protobuf-5.28.2 pyhocon-0.3.61 websockets-13.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: biopython in /usr/local/lib/python3.10/dist-packages (1.78)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.26.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.54.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.7/163.7 kB\u001b[0m \u001b[31m382.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading fonttools-4.54.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fonttools\n",
      "Successfully installed fonttools-4.54.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting protobuf==3.20\n",
      "  Downloading protobuf-3.20.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (698 bytes)\n",
      "Downloading protobuf-3.20.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m967.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.28.2\n",
      "    Uninstalling protobuf-5.28.2:\n",
      "      Successfully uninstalled protobuf-5.28.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cudf 23.12.0 requires numba<0.58,>=0.57, but you have numba 0.60.0 which is incompatible.\n",
      "cudf 23.12.0 requires numpy<1.25,>=1.21, but you have numpy 1.26.4 which is incompatible.\n",
      "cudf 23.12.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.2.2 which is incompatible.\n",
      "cudf 23.12.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.0 which is incompatible.\n",
      "dask-cudf 23.12.0 requires numpy<1.25,>=1.21, but you have numpy 1.26.4 which is incompatible.\n",
      "dask-cudf 23.12.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.2.2 which is incompatible.\n",
      "nvflare 2.5.0 requires protobuf>=4.24.4, but you have protobuf 3.20.0 which is incompatible.\n",
      "nvidia-pytriton 0.4.0 requires pyzmq~=23.0, but you have pyzmq 26.1.0 which is incompatible.\n",
      "onnx 1.16.0 requires protobuf>=3.20.2, but you have protobuf 3.20.0 which is incompatible.\n",
      "triton-model-navigator 0.7.4 requires mpmath<1.0.0, but you have mpmath 1.3.0 which is incompatible.\n",
      "triton-model-navigator 0.7.4 requires onnx~=1.14.0, but you have onnx 1.16.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed protobuf-3.20.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#%%capture --no-display --no-stderr cell_output\n",
    "! pip install nvflare~=2.5.0rc\n",
    "! pip install biopython\n",
    "! pip install scikit-learn\n",
    "! pip install matplotlib\n",
    "! pip install protobuf==3.20\n",
    "\n",
    "import io\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import requests\n",
    "import split_data\n",
    "\n",
    "from Bio import SeqIO\n",
    "from importlib import reload\n",
    "from nvflare import SimulatorRunner  \n",
    "from split_data import split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9c2167",
   "metadata": {},
   "source": [
    "### Obtaining the protein embeddings using the BioNeMo ESM-1nv model\n",
    "Using BioNeMo, each FL client can obtain numerical vector representations of protein sequences called embeddings. Protein embeddings can then be used for visualization or making downstream predictions.\n",
    "\n",
    "Here we are interested in training a neural network to predict subcellular location from an embedding.\n",
    "\n",
    "The data we will be using comes from the paper [Light attention predicts protein location from the language of life](https://academic.oup.com/bioinformaticsadvances/article/1/1/vbab035/6432029) by Stärk et al. In this paper, the authors developed a machine learning algorithm to predict the subcellular location of proteins from sequence through protein langage models that are similar to those hosted by BioNeMo. Protein subcellular location refers to where the protein localizes in the cell, for example a protein my be expressed in the Nucleus or in the Cytoplasm. Knowing where proteins localize can provide insights into the underlying mechanisms of cellular processes and help identify potential targets for drug development. The following image includes a few examples of subcellular locations in an animal cell:\n",
    "\n",
    "\n",
    "(Image freely available at https://pixabay.com/images/id-48542)\n",
    "\n",
    "### Dataset sourcing\n",
    "For our target input sequences, we will point to FASTA sequences in a benchmark dataset called Fitness Landscape Inference for Proteins (FLIP). FLIP encompasses experimental data across adeno-associated virus stability for gene therapy, protein domain B1 stability and immunoglobulin binding, and thermostability from multiple protein families."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8407b137",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example protein dataset location\n",
    "fasta_url= \"http://data.bioembeddings.com/public/FLIP/fasta/scl/mixed_soft.fasta\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fe63c2",
   "metadata": {},
   "source": [
    "First, we define the source of example protein dataset with the FASTA sequences. This data follows the [biotrainer](https://github.com/sacdallago/biotrainer/blob/main/docs/data_standardization.md) standard, so it includes information about the class in the FASTA header, and the protein sequence. Here are two example sequences in this file:\n",
    "\n",
    "```\n",
    ">Sequence1 TARGET=Cell_membrane SET=train VALIDATION=False\n",
    "MMKTLSSGNCTLNVPAKNSYRMVVLGASRVGKSSIVSRFLNGRFEDQYTPTIEDFHRKVYNIHGDMYQLDILDTSGNHPFPAM\n",
    "RRLSILTGDVFILVFSLDSRESFDEVKRLQKQILEVKSCLKNKTKEAAELPMVICGNKNDHSELCRQVPAMEAELLVSGDENC\n",
    "AYFEVSAKKNTNVNEMFYVLFSMAKLPHEMSPALHHKISVQYGDAFHPRPFCMRRTKVAGAYGMVSPFARRPSVNSDLKYIKA\n",
    "KVLREGQARERDKCSIQ\n",
    ">Sequence4833 TARGET=Nucleus SET=train VALIDATION=False\n",
    "MARTKQTARKSTGGKAPRKQLATKAARKSAPATGGVKKPHRFRPGTVALREIRKYQKSTELLIRKLPFQRLVREIAQDFKTDL\n",
    "RFQSSAVAALQEAAEAYLVGLFEDTNLCAIHAKRVTIMPKDIQLARRIRGERA\n",
    "Note the following attributes in the FASTA header:\n",
    "```\n",
    "\n",
    "* `TARGET` attribute holds the subcellular location classification for the sequence, for instance Cell_membrane and Nucleus. This dataset includes a total of ten subcellelular location classes -- more on that below.\n",
    "* `SET` attribute defines whether the sequence should be used for training (train) or testing (test)\n",
    "* `VALIDATION` attribute defines whether the sequence should be used for validation (all sequences where this is True are also in set=train)\n",
    "\n",
    "### Downloading the protein sequences and subcellular location annotations\n",
    "In this step we download the FASTA file defined above and parse the sequences into a list of BioPython SeqRecord objects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d7bfca4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 13949 sequences\n"
     ]
    }
   ],
   "source": [
    "# Download the FASTA file from FLIP: https://github.com/J-SNACKKB/FLIP/tree/main/splits/scl\n",
    "fasta_content = requests.get(fasta_url, headers={\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x86)'\n",
    "}).content.decode('utf-8')\n",
    "fasta_stream = io.StringIO(fasta_content)\n",
    "\n",
    "# Obtain a list of SeqRecords/proteins which contain sequence and attributes\n",
    "# from the FASTA header\n",
    "proteins = list(SeqIO.parse(fasta_stream, \"fasta\"))\n",
    "print(f\"Downloaded {len(proteins)} sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cecf5338-d6a0-432a-9d77-b0583c708431",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bionemo_home = \"/workspace/bionemo\"\n",
    "os.environ['BIONEMO_HOME'] = bionemo_home"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e480628f",
   "metadata": {},
   "source": [
    "### Download Model Checkpoints\n",
    "\n",
    "In order to download pretrained models from the NGC registry, **please ensure that you have installed and configured the NGC CLI**, check the [Quickstart Guide](https://docs.nvidia.com/bionemo-framework/latest/quickstart-fw.html) for more info. The following code will download the pretrained model `esm2nv_650M_converted.nemo` from the NGC registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4916a4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the NGC CLI API KEY and ORG for the model download\n",
    "# If these variables are not already set in the container, uncomment below\n",
    "# to define and set with your API KEY and ORG\n",
    "# api_key = <YOUR_API_KEY>\n",
    "# ngc_cli_org = <YOUR_ORG_NAME>\n",
    "# # Update the environment variable\n",
    "# os.environ['NGC_CLI_API_KEY'] = api_key\n",
    "# os.environ['NGC_CLI_ORG'] = ngc_cli_org\n",
    "\n",
    "model_name = \"esm1nv\"\n",
    "actual_checkpoint_name = \"esm1nv.nemo\"\n",
    "model_path = os.path.join(bionemo_home, 'models')\n",
    "checkpoint_path = os.path.join(model_path, actual_checkpoint_name)\n",
    "os.environ['MODEL_PATH'] = model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a98dfef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture --no-display --no-stderr cell_output\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    !cd /workspace/bionemo && \\\n",
    "    python download_artifacts.py --model_dir models --models {model_name}\n",
    "else:\n",
    "    print(f\"Model {model_name} already exists at {model_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd955150",
   "metadata": {},
   "source": [
    "### Data splitting\n",
    "Next, we prepare the data for simulating federated learning using `n_clients`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10fbc811",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 8619 valid sequences.\n",
      "Partition protein dataset with 10 classes into 3 sites with Dirichlet sampling under alpha 100.0\n",
      "{'site-1': {'Cell_membrane': 178,\n",
      "            'Cytoplasm': 402,\n",
      "            'Endoplasmic_reticulum': 160,\n",
      "            'Extracellular': 461,\n",
      "            'Golgi_apparatus': 56,\n",
      "            'Lysosome': 51,\n",
      "            'Mitochondrion': 324,\n",
      "            'Nucleus': 587,\n",
      "            'Peroxisome': 30,\n",
      "            'Plastid': 142},\n",
      " 'site-2': {'Cell_membrane': 156,\n",
      "            'Cytoplasm': 354,\n",
      "            'Endoplasmic_reticulum': 140,\n",
      "            'Extracellular': 404,\n",
      "            'Golgi_apparatus': 50,\n",
      "            'Lysosome': 45,\n",
      "            'Mitochondrion': 285,\n",
      "            'Nucleus': 515,\n",
      "            'Peroxisome': 27,\n",
      "            'Plastid': 125},\n",
      " 'site-3': {'Cell_membrane': 180,\n",
      "            'Cytoplasm': 407,\n",
      "            'Endoplasmic_reticulum': 162,\n",
      "            'Extracellular': 467,\n",
      "            'Golgi_apparatus': 58,\n",
      "            'Lysosome': 53,\n",
      "            'Mitochondrion': 329,\n",
      "            'Nucleus': 594,\n",
      "            'Peroxisome': 32,\n",
      "            'Plastid': 145}}\n",
      "Saved 2390 training and 1700 testing proteins for site-1, (10/10) unique train/test classes.\n",
      "Saved 2100 training and 1700 testing proteins for site-2, (10/10) unique train/test classes.\n",
      "Saved 2426 training and 1700 testing proteins for site-3, (10/10) unique train/test classes.\n",
      "Partition protein dataset with 10 classes into 3 sites with Dirichlet sampling under alpha 100.0\n",
      "{'site-1': {'Cell_membrane': 178,\n",
      "            'Cytoplasm': 402,\n",
      "            'Endoplasmic_reticulum': 160,\n",
      "            'Extracellular': 461,\n",
      "            'Golgi_apparatus': 56,\n",
      "            'Lysosome': 51,\n",
      "            'Mitochondrion': 324,\n",
      "            'Nucleus': 587,\n",
      "            'Peroxisome': 30,\n",
      "            'Plastid': 142},\n",
      " 'site-2': {'Cell_membrane': 156,\n",
      "            'Cytoplasm': 354,\n",
      "            'Endoplasmic_reticulum': 140,\n",
      "            'Extracellular': 404,\n",
      "            'Golgi_apparatus': 50,\n",
      "            'Lysosome': 45,\n",
      "            'Mitochondrion': 285,\n",
      "            'Nucleus': 515,\n",
      "            'Peroxisome': 27,\n",
      "            'Plastid': 125},\n",
      " 'site-3': {'Cell_membrane': 180,\n",
      "            'Cytoplasm': 407,\n",
      "            'Endoplasmic_reticulum': 162,\n",
      "            'Extracellular': 467,\n",
      "            'Golgi_apparatus': 58,\n",
      "            'Lysosome': 53,\n",
      "            'Mitochondrion': 329,\n",
      "            'Nucleus': 594,\n",
      "            'Peroxisome': 32,\n",
      "            'Plastid': 145}}\n",
      "Saved 2390 training and 1700 testing proteins for site-1, (10/10) unique train/test classes.\n",
      "Saved 2100 training and 1700 testing proteins for site-2, (10/10) unique train/test classes.\n",
      "Saved 2426 training and 1700 testing proteins for site-3, (10/10) unique train/test classes.\n"
     ]
    }
   ],
   "source": [
    "n_clients = 3\n",
    "# limiting to the proteins with sequence length<512 for embedding queries\n",
    "MAX_SEQUENCE_LEN = 512\n",
    "seed=0\n",
    "out_dir = \"/tmp/data/mixed_soft\"\n",
    "split_alpha = 100.0  # moderate label heterogeneity of alpha=1.0\n",
    "\n",
    "reload(split_data)\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Extract meta data and split\n",
    "data = []\n",
    "for i, x in enumerate(proteins):\n",
    "        if len(str(x.seq)) > MAX_SEQUENCE_LEN:\n",
    "            continue\n",
    "            \n",
    "        entry = {key: value for key, value in re.findall(r\"([A-Z_]+)=(-?[A-z0-9]+[.0-9]*)\", x.description)}\n",
    "        entry[\"sequence\"] = str(x.seq)\n",
    "        entry[\"id\"] = str(i)\n",
    "       \n",
    "        data.append(entry)\n",
    "print(f\"Read {len(data)} valid sequences.\")\n",
    "               \n",
    "# Split the data and save for each client\n",
    "# Note, test_data is kept the same on each client and is not split\n",
    "# `concat=False` is used for SCL experiments (see ../downstream/scl)\n",
    "split(proteins=data, num_sites=n_clients, split_dir=out_dir, alpha=split_alpha, concat=False)  \n",
    "# `concat=True` is used for separate inference + MLP classifier in this notebook\n",
    "split(proteins=data, num_sites=n_clients, split_dir=out_dir, alpha=split_alpha, concat=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fc9110",
   "metadata": {},
   "source": [
    "### Federated embedding extraction\n",
    "Running inference of the ESM-1nv model to extract embeddings requires a GPU with at least 12 GB memory. Here we run inference on each client sequentially using one thread to preserve GPU memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72344e9b",
   "metadata": {},
   "source": [
    "First, copy the model into the job folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71854a35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: cannot stat '/workspace/bionemo/models/esm1nv.nemo': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cp /workspace/bionemo/models/esm1nv.nemo jobs/embeddings/app/models/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8073e435",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-27 11:00:39,783 - SimulatorRunner - INFO - Create the Simulator Server.\n",
      "2024-09-27 11:00:39,787 - CoreCell - INFO - server: creating listener on tcp://0:37423\n",
      "2024-09-27 11:00:39,812 - CoreCell - INFO - server: created backbone external listener for tcp://0:37423\n",
      "2024-09-27 11:00:39,814 - ConnectorManager - INFO - 266: Try start_listener Listener resources: {'secure': False, 'host': 'localhost'}\n",
      "2024-09-27 11:00:39,816 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00002 PASSIVE tcp://0:45745] is starting\n",
      "2024-09-27 11:00:40,318 - CoreCell - INFO - server: created backbone internal listener for tcp://localhost:45745\n",
      "2024-09-27 11:00:40,321 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00001 PASSIVE tcp://0:37423] is starting\n",
      "2024-09-27 11:00:40,383 - nvflare.fuel.hci.server.hci - INFO - Starting Admin Server localhost on Port 50363\n",
      "2024-09-27 11:00:40,384 - SimulatorRunner - INFO - Deploy the Apps.\n",
      "2024-09-27 11:00:41,068 - SimulatorRunner - INFO - Create the simulate clients.\n",
      "2024-09-27 11:00:41,073 - ClientManager - INFO - Client: New client site-1@172.23.217.33 joined. Sent token: f6d521dc-44ac-4a3a-8ef0-3527a94f68c2.  Total clients: 1\n",
      "2024-09-27 11:00:41,075 - FederatedClient - INFO - Successfully registered client:site-1 for project simulator_server. Token:f6d521dc-44ac-4a3a-8ef0-3527a94f68c2 SSID:\n",
      "2024-09-27 11:00:41,079 - ClientManager - INFO - Client: New client site-2@172.23.217.33 joined. Sent token: a1fbc2ba-9e90-46cc-a761-36c5e182882d.  Total clients: 2\n",
      "2024-09-27 11:00:41,080 - FederatedClient - INFO - Successfully registered client:site-2 for project simulator_server. Token:a1fbc2ba-9e90-46cc-a761-36c5e182882d SSID:\n",
      "2024-09-27 11:00:41,083 - ClientManager - INFO - Client: New client site-3@172.23.217.33 joined. Sent token: 00df5d92-7365-4b3d-a7ba-1556e2982dea.  Total clients: 3\n",
      "2024-09-27 11:00:41,086 - FederatedClient - INFO - Successfully registered client:site-3 for project simulator_server. Token:00df5d92-7365-4b3d-a7ba-1556e2982dea SSID:\n",
      "2024-09-27 11:00:41,087 - SimulatorRunner - INFO - Set the client status ready.\n",
      "2024-09-27 11:00:41,088 - SimulatorRunner - INFO - Deploy and start the Server App.\n",
      "2024-09-27 11:00:41,091 - Cell - INFO - Register blob CB for channel='server_command', topic='*'\n",
      "2024-09-27 11:00:41,092 - Cell - INFO - Register blob CB for channel='aux_communication', topic='*'\n",
      "2024-09-27 11:00:41,092 - ServerCommandAgent - INFO - ServerCommandAgent cell register_request_cb: server.simulate_job\n",
      "2024-09-27 11:00:41,132 - AuxRunner - INFO - registered aux handler for topic __sync_runner__\n",
      "2024-09-27 11:00:41,133 - AuxRunner - INFO - registered aux handler for topic __job_heartbeat__\n",
      "2024-09-27 11:00:41,134 - AuxRunner - INFO - registered aux handler for topic __task_check__\n",
      "2024-09-27 11:00:41,136 - AuxRunner - INFO - registered aux handler for topic RM.RELIABLE_REQUEST\n",
      "2024-09-27 11:00:41,137 - AuxRunner - INFO - registered aux handler for topic RM.RELIABLE_REPLY\n",
      "2024-09-27 11:00:41,138 - ReliableMessage - INFO - enabled reliable message: max_request_workers=20 query_interval=2.0\n",
      "2024-09-27 11:00:41,139 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job]: Server runner starting ...\n",
      "2024-09-27 11:00:41,140 - AuxRunner - INFO - registered aux handler for topic fed.event\n",
      "2024-09-27 11:00:41,141 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job]: starting workflow bionemo_inference (<class 'bionemo_inference.BioNeMoInference'>) ...\n",
      "2024-09-27 11:00:41,142 - BioNeMoInference - INFO - [identity=simulator_server, run=simulate_job, wf=bionemo_inference]: Initializing BroadcastAndProcess.\n",
      "2024-09-27 11:00:41,143 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=bionemo_inference]: Workflow bionemo_inference (<class 'bionemo_inference.BioNeMoInference'>) started\n",
      "2024-09-27 11:00:41,162 - BioNeMoInferenceProcessor - INFO - [identity=simulator_server, run=simulate_job, wf=bionemo_inference]: Load model configuration from /workspace/bionemo/examples/conf/base_infer_config.yaml and /tmp/nvflare/bionemo/embeddings/server/simulate_job/app_server/config/infer.yaml\n",
      "2024-09-27 11:00:41,165 - WFCommServer - INFO - [identity=simulator_server, run=simulate_job, wf=bionemo_inference]: scheduled task bionemo_inference\n",
      "2024-09-27 11:00:42,091 - SimulatorClientRunner - INFO - Start the clients run simulation.\n",
      "2024-09-27 11:00:43,095 - SimulatorClientRunner - INFO - Simulate Run client: site-1 on GPU group: None\n",
      "2024-09-27 11:00:44,191 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00006 127.0.0.1:37423 <= 127.0.0.1:58130] is created: PID: 266\n",
      "2024-09-27 11:00:44,113 - ClientTaskWorker - INFO - ClientTaskWorker started to run\n",
      "2024-09-27 11:00:44,184 - CoreCell - INFO - site-1.simulate_job: created backbone external connector to tcp://localhost:37423\n",
      "2024-09-27 11:00:44,184 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00001 ACTIVE tcp://localhost:37423] is starting\n",
      "2024-09-27 11:00:44,188 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00002 127.0.0.1:58130 => 127.0.0.1:37423] is created: PID: 356\n",
      "2024-09-27 11:00:45,460 - matplotlib.font_manager - INFO - generated new fontManager\n"
     ]
    }
   ],
   "source": [
    "simulator = SimulatorRunner(\n",
    "    job_folder=\"jobs/embeddings\",\n",
    "    workspace=\"/tmp/nvflare/bionemo/embeddings\",\n",
    "    n_clients=n_clients,\n",
    "    threads=1  # due to memory constraints, we run the client execution sequentially in one thread\n",
    ")\n",
    "run_status = simulator.run()\n",
    "print(\"Simulator finished with run_status\", run_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6448e76d",
   "metadata": {},
   "source": [
    "### Inspecting the embeddings and labels\n",
    "Embeddings returned from the BioNeMo model are vectors of fixed size for each input sequence. In other words, if we input 10 sequences, we will obtain a matrix `10xD`, where `D` is the size of the embedding (in the case of ESM-1nv, `D=768`). At a glance, these real-valued vector embeddings don't show any obvious features (see the printout in the next cell). But these vectors do contain information that can be used in downstream models to reveal properties of the protein, for example the subcellular location as we'll explore below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cb283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings from site-1\n",
    "protein_embeddings = pickle.load(open(os.path.join(out_dir, \"data_site-1.pkl\"), \"rb\"))\n",
    "print(f\"Loaded {len(protein_embeddings)} embeddings from site-1.\")\n",
    "\n",
    "for i in range(4):\n",
    "    protein_embedding = protein_embeddings[i]\n",
    "    print(f\"Inference result contains {list(protein_embedding.keys())}\")\n",
    "    x = protein_embedding[\"embeddings\"]\n",
    "    print(f\"{protein_embedding['id']}: range {np.min(x)}-{np.max(x)}, mean={np.mean(x)}, shape={x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1bb7dc",
   "metadata": {},
   "source": [
    "Let's enumerate the labels corresponding to potential subcellular locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d0cd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also print all the labels\n",
    "\n",
    "labels = set([entry['TARGET'] for entry in data])\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    print(f\"{i+1}. {label.replace('_', ' ')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d9eefb",
   "metadata": {},
   "source": [
    "### Training a MLP to predict subcellular location\n",
    "To be able to classify proteins for their subcellular location, we train a simple scikit-learn Multi-layer Perceptron (MPL) classifier using Federated Averaging ([FedAvg](https://arxiv.org/abs/1602.05629)). The MLP model uses a network of hidden layers to fit the input embedding vectors to the model classes (the cellular locations above). In the simulation below, we define the MLP to use the Adam optimizer with a network of (512, 256, 128) hidden layers, defining a random state (or seed) for reproducibility, and trained for 30 rounds of FedAvg (see [config_fed_server.json](./jobs/fedavg/app/config/config_fed_server.json)). \n",
    "\n",
    "We can use the same configuration also to simulate local training where each client is only training with their own data by setting `os.environ[\"SIM_LOCAL\"] = \"True\"`. Our [BioNeMoMLPLearner](./jobs/fedavg/app/custom/bionemo_mlp_learner.py) will then ignore the global weights coming from the server.\n",
    "\n",
    "### Local training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72a9dad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"SIM_LOCAL\"] = \"True\"\n",
    "\n",
    "simulator = SimulatorRunner(\n",
    "    job_folder=\"jobs/fedavg\",\n",
    "    workspace=f\"/tmp/nvflare/bionemo/local_alpha{split_alpha}\",\n",
    "    n_clients=n_clients,\n",
    "    threads=n_clients\n",
    ")\n",
    "run_status = simulator.run()\n",
    "print(\"Simulator finished with run_status\", run_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b58f1f5",
   "metadata": {},
   "source": [
    "### Federated learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67275cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SIM_LOCAL\"] = \"False\"\n",
    "\n",
    "simulator = SimulatorRunner(\n",
    "    job_folder=\"jobs/fedavg\",\n",
    "    workspace=f\"/tmp/nvflare/bionemo/fedavg_alpha{split_alpha}\",\n",
    "    n_clients=n_clients,\n",
    "    threads=n_clients\n",
    ")\n",
    "run_status = simulator.run()\n",
    "print(\"Simulator finished with run_status\", run_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791d6304",
   "metadata": {},
   "source": [
    "### TensorBoard Visualization\n",
    "You can visualize the training progress using TensorBoard\n",
    "```\n",
    "tensorboard --logdir /tmp/nvflare/bionemo\n",
    "```\n",
    "\n",
    "An example of local (red) vs federated (blue) training is shown below.\n",
    "\n",
    "![TensorBoard training curves](tb_curve.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
